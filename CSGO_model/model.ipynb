{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Variational Auto Encoder\n",
    "The logic for this has been taken from the original paper and Lecture 13 of CS231n"
   ],
   "id": "213c12971f5226a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "a3f8c3e5ff7df073"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:05:11.177967Z",
     "start_time": "2025-06-21T12:05:05.020099Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import tqdm"
   ],
   "id": "7bf7ca077d935873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:14:58.714599Z",
     "start_time": "2025-06-21T12:14:58.704519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "\n",
    "        self.img2hid = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hid2mu = nn.Linear(hidden_dim, output_dim)\n",
    "        self.hid2var = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.z2hid = nn.Linear(output_dim, hidden_dim)\n",
    "        self.hid2img = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "\n",
    "        h = self.relu(self.img2hid(x))\n",
    "        mu = self.hid2mu(h)\n",
    "        logvar = self.hid2var(h)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "\n",
    "        h = self.relu(self.z2hid(z))\n",
    "\n",
    "        return self.sigmoid(self.hid2img(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mu, sigma = self.encode(x)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z_recreated = mu + epsilon * sigma\n",
    "\n",
    "        return z_recreated, mu, torch.log(sigma)"
   ],
   "id": "e9ecf2b8ef64d1b0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading dataset\n",
    "\n",
    "This block is mostly AI generated."
   ],
   "id": "5d64d640bb5c0fb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:19:01.132119Z",
     "start_time": "2025-06-21T12:19:01.128120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ],
   "id": "e53fcb7991eb450f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:19:02.674429Z",
     "start_time": "2025-06-21T12:19:02.666923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_image_dataloader(root_dir='./dataset', batch_size=16, target_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for unlabeled images optimized for CUDA\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Directory containing JPG images\n",
    "        batch_size (int): Batch size for DataLoader\n",
    "        target_size (tuple): Target size for image resizing (H, W)\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader ready for training\n",
    "        dict: Dataset information dictionary\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    class UnlabeledImageDataset(Dataset):\n",
    "        def __init__(self, root_dir, transform=None, target_size=target_size):\n",
    "            self.root_dir = Path(root_dir)\n",
    "            self.transform = transform\n",
    "            self.target_size = target_size\n",
    "\n",
    "            # Collect all JPG images\n",
    "            self.image_paths = list(self.root_dir.glob('*.jpg'))\n",
    "\n",
    "            if not self.image_paths:\n",
    "                raise RuntimeError(f\"No JPG images found in {root_dir}\")\n",
    "\n",
    "            print(f\"Found {len(self.image_paths)} images in dataset\")\n",
    "\n",
    "            # Pre-cache images if using CUDA\n",
    "            self.cache = {}\n",
    "            if device.type == 'cuda':\n",
    "                print(\"Pre-caching images for GPU acceleration...\")\n",
    "                for i in range(len(self.image_paths)):\n",
    "                    self.__getitem__(i)\n",
    "                print(\"Image caching completed\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Use cached image if available\n",
    "            if idx in self.cache:\n",
    "                return self.cache[idx]\n",
    "\n",
    "            img_path = self.image_paths[idx]\n",
    "\n",
    "            try:\n",
    "                # Load image and ensure RGB format\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "                # Apply transformations if specified\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "\n",
    "                # Cache the image tensor\n",
    "                self.cache[idx] = image\n",
    "                return image\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "                # Return a blank image if there's an error\n",
    "                blank = torch.zeros(3, *self.target_size)\n",
    "                self.cache[idx] = blank\n",
    "                return blank\n",
    "\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),          # Resize to target size\n",
    "        transforms.ToTensor(),                   # Convert to tensor [0, 1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = UnlabeledImageDataset(\n",
    "        root_dir=root_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create data loader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for Windows compatibility\n",
    "        pin_memory=(device.type == 'cuda')  # Pin memory for faster GPU transfers\n",
    "    )\n",
    "\n",
    "    # Prepare dataset information\n",
    "    dataset_info = {\n",
    "        'num_images': len(dataset),\n",
    "        'image_paths': [str(p) for p in dataset.image_paths],\n",
    "        'original_size': (1920, 1080),\n",
    "        'processed_size': target_size,\n",
    "        'device': str(device),\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "\n",
    "    print(\"\\nDataLoader created successfully\")\n",
    "    print(f\"  Total images: {len(dataset)}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Number of batches: {len(dataloader)}\")\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"  CUDA Memory Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "\n",
    "    return dataloader, dataset_info"
   ],
   "id": "c5612e8ce75a487",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "c3547e0959c39012"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:32:02.817326Z",
     "start_time": "2025-06-21T12:32:02.813855Z"
    }
   },
   "cell_type": "code",
   "source": "from tqdm import tqdm",
   "id": "a8a45b5a27e77693",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:22:31.138864Z",
     "start_time": "2025-06-21T12:22:11.179070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VariationalAutoencoder(256*256, 128*128, 32*32).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ],
   "id": "e1a001df09962298",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:23:23.216403Z",
     "start_time": "2025-06-21T12:23:23.206578Z"
    }
   },
   "cell_type": "code",
   "source": "num_epochs = 10",
   "id": "77deafffda6e8a95",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:32:20.089472Z",
     "start_time": "2025-06-21T12:32:05.458082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataloader, dataset_info = create_image_dataloader()\n",
    "\n",
    "    torch.save(dataset_info, \"./dataset/dataset_info.pth\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        iter = tqdm(dataloader)\n",
    "        for batch_idx, (data, _) in enumerate(iter):\n",
    "\n",
    "            data = data.to(device).reshape(-1, 3, 256, 256)\n",
    "            output, mu, logvar = model(data)\n",
    "\n",
    "            reconstruction_loss = criterion(output, data)\n",
    "            kl_loss = -torch.sum(1 + 2*logvar - mu.pow(2) - logvar.exp()/2)\n",
    "\n",
    "            net_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            net_loss.backward()\n",
    "            optimizer.step()\n",
    "            iter.set_postfix(loss=net_loss.item())\n",
    "\n"
   ],
   "id": "1f7a78aad5170fe4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Total GPU Memory: 8.00 GB\n",
      "Found 748 images in dataset\n",
      "Pre-caching images for GPU acceleration...\n",
      "Image caching completed\n",
      "\n",
      "DataLoader created successfully\n",
      "  Total images: 748\n",
      "  Batch size: 16\n",
      "  Number of batches: 47\n",
      "  CUDA Memory Allocated: 8384.38 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m=\u001B[39m tqdm(dataloader)\n\u001B[1;32m----> 9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (data, _) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28miter\u001B[39m):\n\u001B[0;32m     11\u001B[0m         data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m256\u001B[39m)\n\u001B[0;32m     12\u001B[0m         output, mu, logvar \u001B[38;5;241m=\u001B[39m model(data)\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5caf7151b711ce06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
